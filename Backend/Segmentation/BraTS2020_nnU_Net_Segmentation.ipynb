{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup & Environment","metadata":{}},{"cell_type":"code","source":"import os, glob, random, logging, sys, time\nimport numpy as np\nimport nibabel as nib\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nlogging.basicConfig(level=logging.INFO, format=\"%(message)s\", handlers=[logging.StreamHandler(sys.stdout)])\nLOG = logging.getLogger(\"BraTS_Metrics\")\n\nSEED, DEVICE = 42, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\nPATCH_SIZE, BATCH_SIZE, EPOCHS, LR_START = (128, 128, 128), 12, 2, 0.01\nTRAIN_BASE = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T14:58:14.781209Z","iopub.execute_input":"2026-01-23T14:58:14.781415Z","iopub.status.idle":"2026-01-23T14:58:21.524119Z","shell.execute_reply.started":"2026-01-23T14:58:14.781394Z","shell.execute_reply":"2026-01-23T14:58:21.523316Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 2. Data Discovery","metadata":{}},{"cell_type":"code","source":"def find_cases(base):\n    start_time = time.time()\n    cases = []\n    found_dirs = sorted([d for d in glob.glob(os.path.join(base, \"BraTS20_Training_*\")) if os.path.isdir(d)])\n    \n    for d in found_dirs:\n        case_id = os.path.basename(d)\n        m = {}\n        missing = False\n        for k in [\"flair\", \"t1\", \"t1ce\", \"t2\", \"seg\"]:\n            files = glob.glob(os.path.join(d, f\"*{k}.nii*\"))\n            if files:\n                m[k] = files[0]\n            else:\n                missing = True\n                break\n        if not missing:\n            cases.append((case_id, m))\n    \n    LOG.info(f\"--- Data Discovery ---\")\n    LOG.info(f\"Total Folders Scanned: {len(found_dirs)}\")\n    LOG.info(f\"Valid Cases Found: {len(cases)}\")\n    LOG.info(f\"Discovery Time: {time.time() - start_time:.2f}s\")\n    return cases\n\ntrain_cases = find_cases(TRAIN_BASE)\ntrain_split, val_split = train_test_split(train_cases, test_size=0.1, random_state=SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T14:58:21.525635Z","iopub.execute_input":"2026-01-23T14:58:21.526052Z","iopub.status.idle":"2026-01-23T14:58:24.065698Z","shell.execute_reply.started":"2026-01-23T14:58:21.526028Z","shell.execute_reply":"2026-01-23T14:58:24.065037Z"}},"outputs":[{"name":"stdout","text":"--- Data Discovery ---\nTotal Folders Scanned: 369\nValid Cases Found: 368\nDiscovery Time: 2.53s\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 3. Dataset Class","metadata":{}},{"cell_type":"code","source":"class BraTSDataset(Dataset):\n    def __init__(self, cases, samples=1000):\n        self.cases = cases\n        self.samples = samples\n\n    def __len__(self): return self.samples\n\n    def __getitem__(self, _):\n        cid, mf = random.choice(self.cases)\n        imgs = []\n        for k in [\"flair\", \"t1\", \"t1ce\", \"t2\"]:\n            data = nib.load(mf[k]).get_fdata().astype(np.float32)\n            mask = data != 0\n            if mask.sum() > 0:\n                data[mask] = (data[mask] - data[mask].mean()) / (data[mask].std() + 1e-8)\n            imgs.append(data)\n        \n        x = np.stack(imgs)\n        seg = nib.load(mf[\"seg\"]).get_fdata()\n        y = np.stack([(seg > 0), ((seg == 1) | (seg == 4)), (seg == 4)]).astype(np.float32)\n\n        d, h, w = x.shape[1:]\n        pd, ph, pw = PATCH_SIZE\n        if random.random() < 0.66:\n            z, i, j = np.where(y[0] > 0)\n            idx = random.randint(0, len(z)-1) if len(z) > 0 else 0\n            sd, sh, sw = np.clip(z[idx]-pd//2, 0, d-pd), np.clip(i[idx]-ph//2, 0, h-ph), np.clip(j[idx]-pw//2, 0, w-pw)\n        else:\n            sd, sh, sw = random.randint(0, d-pd), random.randint(0, h-ph), random.randint(0, w-pw)\n\n        x_p, y_p = x[:, sd:sd+pd, sh:sh+ph, sw:sw+pw], y[:, sd:sd+pd, sh:sh+ph, sw:sw+pw]\n        \n        for axis in [1, 2, 3]:\n            if random.random() > 0.5:\n                x_p, y_p = np.flip(x_p, axis).copy(), np.flip(y_p, axis).copy()\n        \n        return torch.from_numpy(x_p), torch.from_numpy(y_p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T14:58:24.066836Z","iopub.execute_input":"2026-01-23T14:58:24.067119Z","iopub.status.idle":"2026-01-23T14:58:24.076131Z","shell.execute_reply.started":"2026-01-23T14:58:24.067097Z","shell.execute_reply":"2026-01-23T14:58:24.075449Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 4. Model Architecture & Loss","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, i, o):\n        super().__init__()\n        self.c = nn.Sequential(nn.Conv3d(i, o, 3, 1, 1, bias=False), nn.InstanceNorm3d(o), nn.LeakyReLU(0.01, True))\n    def forward(self, x): return self.c(x)\n\nclass nnUNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        f = [32, 64, 128, 256]\n        self.e1, self.e2, self.e3 = ConvBlock(4, f[0]), ConvBlock(f[0], f[1]), ConvBlock(f[1], f[2])\n        self.pool = nn.MaxPool3d(2)\n        self.bottleneck = ConvBlock(f[2], f[3])\n        self.u3, self.u2, self.u1 = nn.ConvTranspose3d(f[3], f[2], 2, 2), nn.ConvTranspose3d(f[2], f[1], 2, 2), nn.ConvTranspose3d(f[1], f[0], 2, 2)\n        self.d3, self.d2, self.d1 = ConvBlock(f[3], f[2]), ConvBlock(f[2], f[1]), ConvBlock(f[1], f[0])\n        self.out, self.ds2, self.ds3 = nn.Conv3d(f[0], 3, 1), nn.Conv3d(f[1], 3, 1), nn.Conv3d(f[2], 3, 1)\n\n    def forward(self, x):\n        s1 = self.e1(x); s2 = self.e2(self.pool(s1)); s3 = self.e3(self.pool(s2))\n        b = self.bottleneck(self.pool(s3))\n        d3 = self.d3(torch.cat([self.u3(b), s3], 1))\n        d2 = self.d2(torch.cat([self.u2(d3), s2], 1))\n        d1 = self.d1(torch.cat([self.u1(d2), s1], 1))\n        return self.out(d1), self.ds2(d2), self.ds3(d3)\n\ndef hybrid_loss(outputs, target):\n    def region_loss(p, y):\n        p_sig = torch.sigmoid(p)\n        dice = 1 - (2*(p_sig*y).sum()+1e-6)/(p_sig.sum()+y.sum()+1e-6)\n        return dice + F.binary_cross_entropy_with_logits(p, y)\n    return region_loss(outputs[0], target) + 0.5*region_loss(outputs[1], F.interpolate(target, outputs[1].shape[2:], mode='nearest')) + 0.25*region_loss(outputs[2], F.interpolate(target, outputs[2].shape[2:], mode='nearest'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T14:58:24.077283Z","iopub.execute_input":"2026-01-23T14:58:24.077544Z","iopub.status.idle":"2026-01-23T14:58:24.098290Z","shell.execute_reply.started":"2026-01-23T14:58:24.077524Z","shell.execute_reply":"2026-01-23T14:58:24.097520Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 5. Training & Evaluation","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(BraTSDataset(train_split), batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\nval_loader = DataLoader(BraTSDataset(val_split, samples=40), batch_size=1)\n\nmodel = nn.DataParallel(nnUNet()).to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr=LR_START, momentum=0.99, nesterov=True)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda ep: (1 - ep/EPOCHS)**0.9)\nscaler = torch.amp.GradScaler(enabled=True)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    epoch_start = time.time()\n    train_losses = []\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n    for x, y in pbar:\n        optimizer.zero_grad()\n        with torch.amp.autocast(device_type=\"cuda\"):\n            outputs = model(x.to(DEVICE))\n            loss = hybrid_loss(outputs, y.to(DEVICE))\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        train_losses.append(loss.item())\n        pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{optimizer.param_groups[0]['lr']:.5f}\"})\n\n    model.eval()\n    metrics = {\"WT\": [], \"TC\": [], \"ET\": []}\n    with torch.no_grad():\n        for vx, vy in val_loader:\n            v_out, _, _ = model(vx.to(DEVICE))\n            p = (torch.sigmoid(v_out) > 0.5).float().cpu()\n            for i, label in enumerate([\"WT\", \"TC\", \"ET\"]):\n                dice = (2.*(p[:,i]*vy[:,i]).sum()/(p[:,i].sum()+vy[:,i].sum()+1e-6)).item()\n                metrics[label].append(dice)\n\n    LOG.info(f\"\\n--- Epoch {epoch+1} Summary ---\")\n    LOG.info(f\"Mean Loss: {np.mean(train_losses):.4f} | Time: {time.time()-epoch_start:.1f}s\")\n    LOG.info(f\"Dice Scores -> WT: {np.mean(metrics['WT']):.3f} | TC: {np.mean(metrics['TC']):.3f} | ET: {np.mean(metrics['ET']):.3f}\")\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T14:58:24.099242Z","iopub.execute_input":"2026-01-23T14:58:24.099476Z","iopub.status.idle":"2026-01-23T15:21:41.574598Z","shell.execute_reply.started":"2026-01-23T14:58:24.099456Z","shell.execute_reply":"2026-01-23T15:21:41.568666Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 84/84 [10:06<00:00,  7.22s/it, Loss=0.4744, LR=0.01000]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 1 Summary ---\nMean Loss: 1.4074 | Time: 674.1s\nDice Scores -> WT: 0.832 | TC: 0.646 | ET: 0.636\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 84/84 [10:51<00:00,  7.76s/it, Loss=0.5717, LR=0.00536] \n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 2 Summary ---\nMean Loss: 0.4405 | Time: 720.2s\nDice Scores -> WT: 0.847 | TC: 0.753 | ET: 0.761\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 6. Save Metrics","metadata":{}},{"cell_type":"code","source":"save_path = \"/kaggle/working/nnunet_brats.pth\"\nsave_start = time.time()\ntorch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), save_path)\n\nLOG.info(f\"--- Model Saving ---\")\nLOG.info(f\"Saved to: {save_path}\")\nLOG.info(f\"File Size: {os.path.getsize(save_path)/1024**2:.2f} MB\")\nLOG.info(f\"Save Latency: {time.time()-save_start:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T15:21:41.593982Z","iopub.execute_input":"2026-01-23T15:21:41.594690Z","iopub.status.idle":"2026-01-23T15:21:41.794427Z","shell.execute_reply.started":"2026-01-23T15:21:41.594645Z","shell.execute_reply":"2026-01-23T15:21:41.793741Z"}},"outputs":[{"name":"stdout","text":"--- Model Saving ---\nSaved to: /kaggle/working/nnunet_brats.pth\nFile Size: 10.20 MB\nSave Latency: 0.18s\n","output_type":"stream"}],"execution_count":6}]}